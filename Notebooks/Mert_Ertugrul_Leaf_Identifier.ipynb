{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from geopy.distance import geodesic, great_circle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier, LogisticRegression, \\\n",
    "    DecisionTreeClassifier, RandomForestClassifier, NaiveBayes, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit,ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"HW2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "        .option(\"delimiter\",\",\")\\\n",
    "        .csv('leaf/leaf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
      "|_c0|_c1|    _c2|   _c3|    _c4|    _c5|    _c6|    _c7|      _c8|      _c9|     _c10|    _c11|     _c12|     _c13|     _c14|   _c15|\n",
      "+---+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
      "|  1|  1|0.72694|1.4742|0.32396|0.98535|    1.0|0.83592|0.0046566|0.0039465|  0.04779| 0.12795| 0.016108|0.0052323|2.7477E-4| 1.1756|\n",
      "|  1|  2|0.74173|1.5257|0.36116|0.98152|0.99825|0.79867|0.0052423|0.0050016|  0.02416|0.090476|0.0081195| 0.002708|7.4846E-5|0.69659|\n",
      "|  1|  3|0.76722|1.5725|0.38998|0.97755|    1.0|0.80812|0.0074573| 0.010121| 0.011897|0.057445|0.0032891|9.2068E-4|3.7886E-5|0.44348|\n",
      "|  1|  4|0.73797|1.4597|0.35376|0.97566|    1.0|0.81697|0.0068768|0.0086068|  0.01595|0.065491|0.0042707|0.0011544|6.6272E-5|0.58785|\n",
      "|  1|  5|0.82301|1.7707|0.44462|0.97698|    1.0|0.75493| 0.007428| 0.010042|0.0079379|0.045339|0.0020514|5.5986E-4|2.3504E-5|0.34214|\n",
      "|  1|  6|0.72997|1.4892|0.34284|0.98755|    1.0|0.84482|0.0049451|0.0044506| 0.010487|0.058528|0.0034138|0.0011248|2.4798E-5|0.34068|\n",
      "|  1|  7|0.82063|1.7529|0.44458|0.97964|0.99649| 0.7677|0.0059279|0.0063954| 0.018375|0.080587|0.0064523|0.0022713|4.1495E-5|0.53904|\n",
      "|  1|  8|0.77982|1.6215|0.39222|0.98512|0.99825|0.80816|0.0050987|0.0047314| 0.024875|0.089686|0.0079794|0.0024664|1.4676E-4|0.66975|\n",
      "|  1|  9|0.83089|1.8199|0.45693| 0.9824|    1.0|0.77106|0.0060055| 0.006564|0.0072447|0.040616|0.0016469|3.8812E-4|3.2863E-5|0.33696|\n",
      "|  1| 10|0.90631|2.3906|0.58336|0.97683|0.99825|0.66419|0.0084019| 0.012848|0.0070096|0.042347|0.0017901|4.5889E-4|2.8251E-5|0.28082|\n",
      "+---+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see from the information about the data that _c1 column is the number of specimens available, which means how many leaf from that plant the sample which is something like a row number. Therefore, I decided to drop that column since it is not a prameter for the leaf structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
      "|_c0|    _c2|   _c3|    _c4|    _c5|    _c6|    _c7|      _c8|      _c9|     _c10|    _c11|     _c12|     _c13|     _c14|   _c15|\n",
      "+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
      "|  1|0.72694|1.4742|0.32396|0.98535|    1.0|0.83592|0.0046566|0.0039465|  0.04779| 0.12795| 0.016108|0.0052323|2.7477E-4| 1.1756|\n",
      "|  1|0.74173|1.5257|0.36116|0.98152|0.99825|0.79867|0.0052423|0.0050016|  0.02416|0.090476|0.0081195| 0.002708|7.4846E-5|0.69659|\n",
      "|  1|0.76722|1.5725|0.38998|0.97755|    1.0|0.80812|0.0074573| 0.010121| 0.011897|0.057445|0.0032891|9.2068E-4|3.7886E-5|0.44348|\n",
      "|  1|0.73797|1.4597|0.35376|0.97566|    1.0|0.81697|0.0068768|0.0086068|  0.01595|0.065491|0.0042707|0.0011544|6.6272E-5|0.58785|\n",
      "|  1|0.82301|1.7707|0.44462|0.97698|    1.0|0.75493| 0.007428| 0.010042|0.0079379|0.045339|0.0020514|5.5986E-4|2.3504E-5|0.34214|\n",
      "|  1|0.72997|1.4892|0.34284|0.98755|    1.0|0.84482|0.0049451|0.0044506| 0.010487|0.058528|0.0034138|0.0011248|2.4798E-5|0.34068|\n",
      "|  1|0.82063|1.7529|0.44458|0.97964|0.99649| 0.7677|0.0059279|0.0063954| 0.018375|0.080587|0.0064523|0.0022713|4.1495E-5|0.53904|\n",
      "|  1|0.77982|1.6215|0.39222|0.98512|0.99825|0.80816|0.0050987|0.0047314| 0.024875|0.089686|0.0079794|0.0024664|1.4676E-4|0.66975|\n",
      "|  1|0.83089|1.8199|0.45693| 0.9824|    1.0|0.77106|0.0060055| 0.006564|0.0072447|0.040616|0.0016469|3.8812E-4|3.2863E-5|0.33696|\n",
      "|  1|0.90631|2.3906|0.58336|0.97683|0.99825|0.66419|0.0084019| 0.012848|0.0070096|0.042347|0.0017901|4.5889E-4|2.8251E-5|0.28082|\n",
      "+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('_c1')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking For Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0 number of nulls\n",
      "0\n",
      "_c2 number of nulls\n",
      "0\n",
      "_c3 number of nulls\n",
      "0\n",
      "_c4 number of nulls\n",
      "0\n",
      "_c5 number of nulls\n",
      "0\n",
      "_c6 number of nulls\n",
      "0\n",
      "_c7 number of nulls\n",
      "0\n",
      "_c8 number of nulls\n",
      "0\n",
      "_c9 number of nulls\n",
      "0\n",
      "_c10 number of nulls\n",
      "0\n",
      "_c11 number of nulls\n",
      "0\n",
      "_c12 number of nulls\n",
      "0\n",
      "_c13 number of nulls\n",
      "0\n",
      "_c14 number of nulls\n",
      "0\n",
      "_c15 number of nulls\n",
      "0\n"
     ]
    }
   ],
   "source": [
    " columns = df.columns\n",
    "\n",
    "for col in columns:\n",
    "    print(col + \" number of nulls\")\n",
    "    print(df.filter(col + ' is null').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seams like there is no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting Colouns to Decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"_c14\", df[\"_c14\"].cast(\"Decimal(10,10)\"))\n",
    "df=df.withColumn(\"_c13\", df[\"_c13\"].cast(\"Decimal(10,10)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|        _c13|        _c14|\n",
      "+------------+------------+\n",
      "|0.0052323000|0.0002747700|\n",
      "|0.0027080000|0.0000748460|\n",
      "|0.0009206800|0.0000378860|\n",
      "|0.0011544000|0.0000662720|\n",
      "|0.0005598600|0.0000235040|\n",
      "|0.0011248000|0.0000247980|\n",
      "|0.0022713000|0.0000414950|\n",
      "|0.0024664000|0.0001467600|\n",
      "|0.0003881200|0.0000328630|\n",
      "|0.0004588900|0.0000282510|\n",
      "|0.0003087200|0.0000318390|\n",
      "|0.0008164800|0.0001385500|\n",
      "|0.0020648000|0.0002388300|\n",
      "|0.0014887000|0.0000832710|\n",
      "|0.0022383000|0.0002016600|\n",
      "|0.0022541000|0.0001985400|\n",
      "|0.0018929000|0.0001245200|\n",
      "|0.0021199000|0.0002772900|\n",
      "|0.0012274000|0.0001492900|\n",
      "|0.0018832000|0.0002434500|\n",
      "+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_c13', '_c14').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: decimal(10,10) (nullable = true)\n",
      " |-- _c14: decimal(10,10) (nullable = true)\n",
      " |-- _c15: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to change parameters data types to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in columns[1:]:\n",
    "#    df = df.withColumn(col, df[col].cast(FloatType()))\n",
    "#df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assembler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can convert parameters to vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecA = VectorAssembler(inputCols=df.columns[1:],outputCol=\"features\")\n",
    "df = vecA.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.72694,1.4742,0...|\n",
      "|[0.74173,1.5257,0...|\n",
      "|[0.76722,1.5725,0...|\n",
      "|[0.73797,1.4597,0...|\n",
      "|[0.82301,1.7707,0...|\n",
      "|[0.72997,1.4892,0...|\n",
      "|[0.82063,1.7529,0...|\n",
      "|[0.77982,1.6215,0...|\n",
      "|[0.83089,1.8199,0...|\n",
      "|[0.90631,2.3906,0...|\n",
      "|[0.7459,1.4927,0....|\n",
      "|[0.79606,1.6934,0...|\n",
      "|[0.93361,2.7582,0...|\n",
      "|[0.91186,2.4994,0...|\n",
      "|[0.89063,2.2927,0...|\n",
      "|[0.86755,2.009,0....|\n",
      "|[0.91852,2.5247,0...|\n",
      "|[0.88795,2.2038,0...|\n",
      "|[0.85121,1.9548,0...|\n",
      "|[0.89084,2.2979,0...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We do not need to string index our label since iti is already a integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('_c0', 'label')\n",
    "df.select('label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since there is no categorical parameter in the data set we do not need to use dummification or one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide to train and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF, testDF) = df.randomSplit([0.75,0.25], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfC = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trani Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7285714285714285\n",
      "Num Trees :  20\n",
      "Max Depth :  8\n",
      "Max Bins :  45\n",
      "Impurity :   gini\n"
     ]
    }
   ],
   "source": [
    "myParamsTVS = ParamGridBuilder()\\\n",
    "            .addGrid(rfC.numTrees,[15,20])\\\n",
    "            .addGrid(rfC.maxDepth,[6,8,10])\\\n",
    "            .addGrid(rfC.seed,[1234])\\\n",
    "            .addGrid(rfC.maxBins,[15,35,45])\\\n",
    "            .build()\n",
    "\n",
    "validatorTVS = TrainValidationSplit( estimator=rfC,\n",
    "                                  estimatorParamMaps=myParamsTVS,\n",
    "                                  trainRatio=0.75,\n",
    "                                   evaluator = evaluator, seed=1234)\n",
    "\n",
    "bestModelRF = validatorTVS.fit(trainDF)\n",
    "resultDF = bestModelRF.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultDF)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "print(\"Num Trees : \",bestModelRF.bestModel._java_obj.getNumTrees())\n",
    "print(\"Max Depth : \",bestModelRF.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Max Bins : \",bestModelRF.bestModel._java_obj.getMaxBins())\n",
    "print(\"Impurity :  \",bestModelRF.bestModel._java_obj.getImpurity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7714285714285715\n",
      "Num Trees :  23\n",
      "Max Depth :  12\n",
      "Max Bins :  32\n",
      "Impurity :   gini\n"
     ]
    }
   ],
   "source": [
    "myParamsCV = ParamGridBuilder()\\\n",
    "            .addGrid(rfC.numTrees,[19,21,23])\\\n",
    "            .addGrid(rfC.maxDepth,[12,15,17])\\\n",
    "            .addGrid(rfC.seed,[1234])\\\n",
    "            .build()\n",
    "\n",
    "validatorCV = CrossValidator(estimator=rfC,\n",
    "                                  estimatorParamMaps=myParamsCV,\n",
    "                                   evaluator = evaluator,\n",
    "                                    numFolds=4, seed=1234)\n",
    "\n",
    "bestModelRF = validatorCV.fit(trainDF)\n",
    "resultDF = bestModelRF.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultDF)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "print(\"Num Trees : \",bestModelRF.bestModel._java_obj.getNumTrees())\n",
    "print(\"Max Depth : \",bestModelRF.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Max Bins : \",bestModelRF.bestModel._java_obj.getMaxBins())\n",
    "print(\"Impurity :  \",bestModelRF.bestModel._java_obj.getImpurity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6571428571428571\n",
      "Num Trees :  12\n",
      "Max Bins :  25\n",
      "Impurity :   gini\n"
     ]
    }
   ],
   "source": [
    "myParamsTVS = ParamGridBuilder()\\\n",
    "            .addGrid(dt.maxDepth,[8,12,28])\\\n",
    "            .addGrid(dt.maxBins,[10,15,25])\\\n",
    "            .build()\n",
    "\n",
    "validatorTVS = TrainValidationSplit( estimator=dt,\n",
    "                                  estimatorParamMaps=myParamsTVS,\n",
    "                                  trainRatio=0.60,\n",
    "                                   evaluator = evaluator)\n",
    "\n",
    "bestModelDT = validatorTVS.fit(trainDF)\n",
    "resultDF = bestModelDT.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultDF)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "print(\"Num Trees : \",bestModelDT.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Max Bins : \",bestModelDT.bestModel._java_obj.getMaxBins())\n",
    "print(\"Impurity :  \",bestModelDT.bestModel._java_obj.getImpurity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.5857142857142857\n",
      "Num Trees :  15\n",
      "Max Bins :  15\n",
      "Impurity :   gini\n"
     ]
    }
   ],
   "source": [
    "myParamsCV = ParamGridBuilder()\\\n",
    "            .addGrid(dt.maxDepth,[12,15,20])\\\n",
    "            .addGrid(dt.maxBins,[10,15,25])\\\n",
    "            .build()\n",
    "validatorCV = CrossValidator(estimator=dt,\n",
    "                                  estimatorParamMaps=myParamsCV,\n",
    "                                   evaluator = evaluator,\n",
    "                                    numFolds=10\n",
    "                                    )\n",
    "\n",
    "bestModelDT = validatorCV.fit(trainDF)\n",
    "resultDF = bestModelDT.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultDF)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "print(\"Num Trees : \",bestModelDT.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Max Bins : \",bestModelDT.bestModel._java_obj.getMaxBins())\n",
    "print(\"Impurity :  \",bestModelDT.bestModel._java_obj.getImpurity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6428571428571429\n",
      "Max Iter :  50\n",
      "Reg Param :  0.0\n",
      "Elastic Net Param :  0.0\n"
     ]
    }
   ],
   "source": [
    "myParamsTVS = ParamGridBuilder()\\\n",
    "            .addGrid(lr.maxIter,[10,50,100])\\\n",
    "            .addGrid(lr.regParam,[0.0, 0.3, 0.5])\\\n",
    "            .addGrid(lr.elasticNetParam,[0.0,0.5,0.8])\\\n",
    "            .build()\n",
    "\n",
    "validatorTVS = TrainValidationSplit( estimator=lr,\n",
    "                                  estimatorParamMaps=myParamsTVS,\n",
    "                                  trainRatio=0.75,\n",
    "                                   evaluator = evaluator, seed=1234)\n",
    "\n",
    "bestModelLR = validatorTVS.fit(trainDF)\n",
    "resultLR = bestModelLR.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultLR)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "\n",
    "print(\"Max Iter : \",bestModelLR.bestModel._java_obj.getMaxIter())\n",
    "print(\"Reg Param : \",bestModelLR.bestModel._java_obj.getRegParam())\n",
    "print(\"Elastic Net Param : \",bestModelLR.bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7142857142857143\n",
      "Max Iter :  100\n",
      "Reg Param :  0.0\n",
      "Elastic Net Param :  0.0\n"
     ]
    }
   ],
   "source": [
    "myParamsCV = ParamGridBuilder()\\\n",
    "            .addGrid(lr.maxIter,[10,50,100])\\\n",
    "            .addGrid(lr.regParam,[0.0, 0.3, 0.5])\\\n",
    "            .addGrid(lr.elasticNetParam,[0.0,0.5,0.8])\\\n",
    "            .build()\n",
    "\n",
    "validatorCV = CrossValidator(estimator=lr,\n",
    "                                  estimatorParamMaps=myParamsCV,\n",
    "                                   evaluator = evaluator,\n",
    "                                    numFolds=4)\n",
    "\n",
    "bestModelLR = validatorCV.fit(trainDF)\n",
    "resultLR = bestModelLR.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultLR)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "\n",
    "print(\"Max Iter : \",bestModelLR.bestModel._java_obj.getMaxIter())\n",
    "print(\"Reg Param : \",bestModelLR.bestModel._java_obj.getRegParam())\n",
    "print(\"Elastic Net Param : \",bestModelLR.bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.014285714285714285\n",
      "Smooting :  0.0\n"
     ]
    }
   ],
   "source": [
    "myParamsTVS = ParamGridBuilder()\\\n",
    "            .addGrid(nb.smoothing,[0.0, 0.2])\\\n",
    "            .build()\n",
    "validatorTVS = TrainValidationSplit( estimator=nb,\n",
    "                                  estimatorParamMaps=myParamsTVS,\n",
    "                                  trainRatio=0.70,\n",
    "                                   evaluator = evaluator)\n",
    "\n",
    "bestModelNB = validatorTVS.fit(trainDF)\n",
    "resultDF = bestModelNB.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultDF)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "print(\"Smooting : \",bestModelNB.bestModel._java_obj.getSmoothing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.014285714285714285\n",
      "Smooting :  0.0\n"
     ]
    }
   ],
   "source": [
    "myParamsCV = ParamGridBuilder()\\\n",
    "            .addGrid(nb.smoothing,[0.0, 0.2])\\\n",
    "            .build()\n",
    "validatorCV = CrossValidator(estimator=nb,\n",
    "                                  estimatorParamMaps=myParamsCV,\n",
    "                                   evaluator = evaluator,\n",
    "                                    numFolds=4)\n",
    "\n",
    "bestModelNB = validatorCV.fit(trainDF)\n",
    "resultDF = bestModelNB.transform(testDF)\n",
    "\n",
    "result = evaluator.evaluate(resultDF)\n",
    "print(\"Accuracy = \",result)\n",
    "\n",
    "print(\"Smooting : \",bestModelNB.bestModel._java_obj.getSmoothing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
